{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle, gzip\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import sys\n",
    "%matplotlib inline\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sys.path.append('..\\\\vae')\n",
    "from vae import vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with gzip.open('..\\\\resources\\\\mnist.pkl.gz', 'rb') as f:\n",
    "    train, test, val = pickle.load(f, encoding='latin1')\n",
    "    mnist_train = train[0]\n",
    "    mnist_test = test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between a Variational Autoencoder and other autoencoder networks is that it explicitly learns a latent variable representation of the data. \n",
    "Our modeling assumption is that the data (in this case, handwritten digits) comes from a latent variable $z$.\n",
    "\n",
    "$$z \\sim N(0, I)$$\n",
    "\n",
    "We want to find the posterior $P(z \\mid x)$, but the computation is intractable. Other methods such as MCMC are computational expensive. The method we will use is variational inference. We will learn an approximation of the posterior $Q(z \\mid x)$. We achieve by adding a regularization term that minimizes the KL divergence between the encoder output $x \\sim N(\\mu, \\sigma^2 I)$ and the latent variable representation. \n",
    "\n",
    "$$D_{kl}(q || p) = -\\frac{1}{2} \\sum (1 + log(\\sigma^2) - \\mu^2 - \\sigma^2) $$\n",
    "\n",
    "The result is a generative models that can produce new images that are similar to ones it has seen.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class vae(object):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, params, latent_dim = 2):\n",
    "        '''intializes weights matrix and parameters'''\n",
    "\n",
    "        # initialize size of VAE\n",
    "        self.latent_dim = latent_dim\n",
    "        self.encoder_layer_sizes = input_dim + [2]\n",
    "        self.decoder_layer_sizes = [latent_dim] + output_dim[::-1] \n",
    "        self.total_layer_sizes   = input_dim + [2, latent_dim] + output_dim[::-1]\n",
    "       \n",
    "        self.number_encoder_layers = len(self.encoder_layer_sizes) - 1\n",
    "        self.number_decoder_layers = len(self.decoder_layer_sizes) - 1\n",
    "        self.number_total_layers   = len(self.total_layer_sizes) - 1\n",
    "\n",
    "        # intialize weights\n",
    "        np.random.seed(10086)\n",
    "        self.encoder_weights = {}\n",
    "        for i in range(self.number_encoder_layers):\n",
    "            self.encoder_weights[i] = np.random.uniform(-0.1, 0.1, \n",
    "                                                        (self.encoder_layer_sizes[i], \n",
    "                                                         self.encoder_layer_sizes[i+1])) \n",
    "        self.decoder_weights = {}\n",
    "        for i in range(self.number_decoder_layers):\n",
    "            self.decoder_weights[i] = np.random.uniform(-0.1, 0.1, \n",
    "                                                        (self.decoder_layer_sizes[i],\n",
    "                                                         self.decoder_layer_sizes[i+1]))\n",
    "        # set params\n",
    "        self.alpha = params['alpha']\n",
    "        self.max_iter = params['max_iter']\n",
    "        self.activation = params['activation']\n",
    "        self.grad_activation = params['grad_act']\n",
    "        self.reconst_loss = params['loss']\n",
    "        self.reconst_grad = params['grad_loss']\n",
    "        self.mode = params['mode']\n",
    "        \n",
    "        if self.mode is 'autoencoder':\n",
    "            self.loss = self.reconst_loss\n",
    "            self.grad_loss = self.reconst_grad\n",
    "        else:\n",
    "            self.loss = (lambda y, yhat: self.reconst_loss(y, yhat) + self.KLD_loss())\n",
    "            self.grad_loss = self.reconst_grad\n",
    "    \n",
    "    def KLD_loss(self):\n",
    "        '''Kullback–Leibler divergence loss'''\n",
    "        return - 0.5 * np.sum(1 + self.sigma - self.mu**2 - np.exp(self.sigma), axis=-1)\n",
    "    \n",
    "    def KLD_grad(self):\n",
    "        '''Kullback–Leibler divergence loss'''\n",
    "        return np.array([self.mu, 0.5 -0.5 / self.sigma])\n",
    "        \n",
    "    def backprop(self, X, y, yhat):\n",
    "        '''back-propagation algorithm'''\n",
    "        # initialize \n",
    "        grad_decoder = {}\n",
    "        grad_encoder = {}\n",
    "    \n",
    "        # backpropogate error through decoder layers\n",
    "        rev_range = np.arange(self.number_decoder_layers)[::-1]\n",
    "        n = rev_range[0]\n",
    "        \n",
    "        if n == 0:\n",
    "            delta = - self.grad_loss(y, yhat) * self.grad_activation(self.decoder_input[0])\n",
    "            grad_decoder[0] = self.encoder_activation[0].T @ delta / X.shape[0]\n",
    "        else:\n",
    "            delta = - self.grad_loss(y, yhat) * self.grad_activation(self.decoder_input[n])\n",
    "            grad_decoder[n] = self.decoder_activation[n-1].T @ delta / X.shape[0]\n",
    "            \n",
    "            for i in rev_range[1:-1]:\n",
    "                delta = delta @ self.decoder_weights[i+1].T * self.grad_activation(self.decoder_input[i])\n",
    "                grad_decoder[i] = self.decoder_activation[i-1].T @ delta / X.shape[0]\n",
    "            \n",
    "            delta = delta @ self.decoder_weights[1].T * self.grad_activation(self.decoder_input[0])\n",
    "            grad_decoder[0] = self.encoder_activation[1].T @ delta / X.shape[0]\n",
    "\n",
    "        # backpropogate errors through encoder layers\n",
    "        rev_range = np.arange(self.number_encoder_layers)[::-1]\n",
    "        n = rev_range[0]\n",
    "        delta_kld = self.KLD_grad().T * self.grad_activation(self.encoder_input[n])\n",
    "        \n",
    "        if n == 0:\n",
    "            delta = delta @ self.decoder_weights[0].T * self.grad_activation(self.encoder_input[0])\n",
    "            if self.mode is 'vae':\n",
    "                delta = delta + delta_kld\n",
    "            grad_encoder[0] = X.T @ delta / X.shape[0]\n",
    "\n",
    "        else:\n",
    "            delta = delta @ self.decoder_weights[0].T * self.grad_activation(self.encoder_input[n])\n",
    "            if self.mode is 'vae':\n",
    "                delta = delta + delta_kld\n",
    "            grad_encoder[n] = self.encoder_activation[0].T @ delta /X.shape[0]\n",
    "        \n",
    "            for i in rev_range[1:-1]:\n",
    "                delta = delta @ self.encoder_weights[i+1].T * self.grad_activation(self.encoder_input[i])\n",
    "                grad_encoder[i] = self.encoder_activation[i-1].T @ delta /X.shape[0]\n",
    "                \n",
    "            delta = delta @ self.encoder_weights[1].T * self.grad_activation(self.encoder_input[0])\n",
    "            grad_encoder[0] = X.T @ delta /X.shape[0]\n",
    "    \n",
    "        return grad_encoder, grad_decoder \n",
    "            \n",
    "    def feedforward(self, train_data):\n",
    "        '''feedforward update step'''\n",
    "        \n",
    "        # initialize storage for activations\n",
    "        self.encoder_input = {}\n",
    "        self.encoder_activation = {}\n",
    "        self.decoder_input = {}\n",
    "        self.decoder_activation = {}\n",
    "        \n",
    "        self.encoder_input[0]      = train_data @ self.encoder_weights[0]\n",
    "        self.encoder_activation[0] = self.activation(self.encoder_input[0])\n",
    "            \n",
    "        # feedforward update on encoder network\n",
    "        for i in range(1, self.number_encoder_layers):\n",
    "            self.encoder_input[i] = self.encoder_input[i-1] @ self.encoder_weights[i]\n",
    "            self.encoder_activation[i] = self.activation(self.encoder_input[i])\n",
    "        \n",
    "        # store output as encoded latent variable parameters\n",
    "        self.mu = self.encoder_activation[self.number_encoder_layers - 1][:,1]\n",
    "        self.sigma = self.encoder_activation[self.number_encoder_layers - 1][:,0]\n",
    "        \n",
    "        # sample latent variable using reparameterization trick\n",
    "        if self.mode is 'vae':\n",
    "            self.epsilon = np.random.normal(0, 1, (self.mu.size, self.latent_dim))\n",
    "            self.z = self.mu[:, None] + self.sigma[:,None] * self.epsilon\n",
    "        else:\n",
    "            self.z = self.encoder_activation[self.number_encoder_layers - 1]\n",
    "    \n",
    "        # feedforward update on the decoder network\n",
    "        self.decoder_input[0]      = self.z @ self.decoder_weights[0]\n",
    "        self.decoder_activation[0] = self.activation(self.decoder_input[0])\n",
    "        \n",
    "        for i in range(1, self.number_decoder_layers):\n",
    "            self.decoder_input[i] = self.decoder_input[i-1] @ self.decoder_weights[i]\n",
    "            self.decoder_activation[i] = self.activation(self.decoder_input[i])\n",
    "\n",
    "        return self.decoder_activation[self.number_decoder_layers - 1]\n",
    "    \n",
    "    \n",
    "    def train(self, X, y):\n",
    "        '''trains the VAE model'''\n",
    "        for i in range(self.max_iter):\n",
    "            yhat = self.feedforward(X)\n",
    "            grad_encoder, grad_decoder = self.backprop(X, y, yhat)\n",
    "            for i in range(self.number_decoder_layers):\n",
    "                self.decoder_weights[i] -= self.alpha * grad_decoder[i]\n",
    "        \n",
    "            for j in range(self.number_encoder_layers):\n",
    "                self.encoder_weights[j] -= self.alpha * grad_encoder[j]\n",
    "                change = grad_encoder[j]\n",
    "                \n",
    "        return None\n",
    "\n",
    "    def predict(self, X):\n",
    "        '''predicts on a trained VAE model'''        \n",
    "        return self.feedforward(X)\n",
    "    \n",
    "    def generate(self, z):\n",
    "        '''generates new images from a trained VAE model'''        \n",
    "        # feedforward on decoder\n",
    "        self.gen_input = {}\n",
    "        self.gen_activation = {}\n",
    "        self.gen_input[0]     = z.T @ self.decoder_weights[0]\n",
    "        self.gen_activation[0] = self.activation(self.gen_input[0])\n",
    "        \n",
    "        for i in range(1, self.number_decoder_layers):\n",
    "            self.gen_input[i] = self.gen_input[i-1] @ self.decoder_weights[i]\n",
    "            self.gen_activation[i] = self.activation(self.gen_input[i])\n",
    "\n",
    "        return self.gen_activation[self.number_decoder_layers - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'alpha' : 0.2,\n",
    "    'max_iter' : 100,\n",
    "    'activation' : (lambda x: 1 / (1 + np.exp(-x))),\n",
    "    'grad_act' : (lambda x: np.exp(-x) / (1 + np.exp(-x))**2),\n",
    "    'loss' : (lambda y, yhat: 0.5 * np.sum((y - yhat)**2)),\n",
    "    'grad_loss' : (lambda y, yhat: y - yhat),\n",
    "    'mode' : 'vae'\n",
    "}\n",
    "\n",
    "example = vae([784, 200], [784, 200], params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'vae' object has no attribute 'mu'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-ab356f3da0ca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mexample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmu\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'vae' object has no attribute 'mu'"
     ]
    }
   ],
   "source": [
    "example.mu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we pass in a set of training digits. From there the network will reconstruct a latent variable representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-b307be117ef3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mexample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmnist_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmnist_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-19-1379fba83415>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    145\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m             \u001b[0myhat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeedforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 147\u001b[1;33m             \u001b[0mgrad_encoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_decoder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackprop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myhat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    148\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumber_decoder_layers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder_weights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malpha\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mgrad_decoder\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-19-1379fba83415>\u001b[0m in \u001b[0;36mbackprop\u001b[1;34m(self, X, y, yhat)\u001b[0m\n\u001b[0;32m     66\u001b[0m             \u001b[0mgrad_decoder\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder_activation\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mdelta\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m             \u001b[0mdelta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myhat\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad_activation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder_input\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m             \u001b[0mgrad_decoder\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder_activation\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mdelta\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "example.train(mnist_train, mnist_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's inspect the generated images compared to the originals. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,3, figsize = (10, 6))\n",
    "\n",
    "for i in range(3):\n",
    "    in_digit = mnist_test[i][None,:]\n",
    "    out_digit = example.predict(in_digit)\n",
    "    ax[0,i].matshow(in_digit.reshape((28,28)),  cmap='gray', clim=(0,1))\n",
    "    ax[1,i].matshow(out_digit.reshape((28,28)), cmap='gray', clim=(0,1))\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now generate new images as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAFpCAYAAACFwHNsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x178b189dd68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(2,2, figsize = (6, 6))\n",
    "\n",
    "a = np.array([1, 3])\n",
    "b = np.array([1, 3])\n",
    "\n",
    "for i, z1 in enumerate(a):\n",
    "    for j, z2 in enumerate(b):\n",
    "        ax[i,j].matshow(example.generate(np.array([z1,z2])).reshape((28,28)),  cmap='gray', clim=(0,1))\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "1. [MNIST data](http://deeplearning.net/tutorial/gettingstarted.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
